{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d0cf131-ed0b-45a8-a62e-536d6c93c244",
   "metadata": {},
   "source": [
    "# trying doc splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca4a019e-7c89-45d0-a53c-7189e03d4a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFium2Loader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7250627-583a-45f9-9e9a-7001888bbfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFium2Loader(\"c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c223e1ec-f659-4bd9-a266-f446aba1a67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 93.8 ms\n",
      "Wall time: 200 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1df848b5-35b8-485d-bb56-85994faae39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splitted_documents = documents #text_splitter.split_documents(documents)\n",
    "len(splitted_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8862bc1d-20d3-4745-a7d1-a7603e80d2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Metadata'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Metadata\"\n",
    "# print([d.metadata['page'] for d in splitted_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f3aaa0-0e67-45c2-a46f-5f8001906929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96808af0-7858-49da-9bd7-ece531913417",
   "metadata": {},
   "source": [
    "# chroma db embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22cdfcc4-d8ba-4e48-bdc7-8d4b70602377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import os\n",
    "import dotenv;dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "476a0516-9099-4d01-83cd-523e63062bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings=OpenAIEmbeddings(openai_api_key=os.environ['OPENAI_API_KEY'])\n",
    "#or\n",
    "# from langchain.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "# embeddings = GPT4AllEmbeddings()\n",
    "\n",
    "# or\n",
    "\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=os.environ['HUGGINGFACEHUB_API_TOKEN'],\n",
    "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
    ")\n",
    "# texts = [\"Hello, world!\", \"How are you?\"]\n",
    "# embeddings.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea0b7eb2-f7bf-4912-bca3-06d86759ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cb284cf-0232-420a-8245-7a0265569053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydash as py_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd3199e8-b914-4200-9c59-d88785fb3b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coll1 = Chroma(\"Coll1\",embeddings,persist_directory=\"db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cc8c869-2246-4b57-affd-babb88012254",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n"
     ]
    }
   ],
   "source": [
    "for chunk in py_.chunk(splitted_documents,20):\n",
    "    coll1.add_documents(chunk)\n",
    "    print([d.metadata['page'] for d in chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c7d58a2-8be7-4a69-bbd0-35dea083074f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# coll1.add_documents(splitted_documents[:10])\n",
    "# db = Chroma.from_documents(splitted_documents, embeddings).as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "210a1b6f-0c45-430c-b55e-5b97a8148034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coll1.get()['ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd1b9ec9-3722-4b49-813f-da3fd7ea60f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 99 is greater than number of elements in index 19, updating n_results = 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 7, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'} -0.396153163947377\n",
      "{'page': 4, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'} -0.3962358573417164\n",
      "{'page': 16, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'} -0.4270880918396964\n",
      "{'page': 12, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'} -0.43081131466587674\n",
      "{'page': 2, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'} -0.43374072888026216\n",
      "{'page': 13, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'} -0.4653879063565991\n",
      "{'page': 9, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'} -0.47782168123815016\n",
      "{'page': 5, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'} -0.48652607265663383\n",
      "{'page': 18, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'} -0.488194952907429\n",
      "{'page': 17, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'} -0.4938289530694018\n",
      "{'page': 1, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'} -0.4951313032480642\n",
      "{'page': 15, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'} -0.5181839455955206\n",
      "{'page': 10, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'} -0.5195032772863446\n",
      "{'page': 8, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'} -0.5352318938131391\n",
      "{'page': 0, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'} -0.5364315177987284\n",
      "{'page': 14, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'} -0.5471728672729328\n",
      "{'page': 3, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'} -0.5567319046077588\n",
      "{'page': 11, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'} -0.5735935454549406\n",
      "{'page': 6, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'} -0.6116797233037645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\sw\\py311nb_env\\Lib\\site-packages\\langchain\\schema\\vectorstore.py:313: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content=\"8\\r\\nLooking ahead, while we continue to refine our plans as we progress throughout the year, we currently \\r\\nexpect total capital expenditures to grow in 2024, driven by our investments across both data centers \\r\\nand servers, particularly in support of our AI work.\\r\\nOn to tax. Absent any changes to U.S. tax law, we expect the tax rate for the rest of the year to be \\r\\nsimilar to Q2 2023.\\r\\nIn closing, Q2 was a good quarter for our business. We’re executing well across our core priorities and \\r\\nare continuing to make progress on delivering exciting new experiences for our community.\\r\\nWith that, Dave, let’s open up the call for questions.\\r\\nOperator: Your first question comes from the line of Brian Nowak with Morgan Stanley.\\r\\nBrian Nowak: Maybe a two-parter for Mark. Mark, you've spoken a few times over the last few \\r\\nmonths on AI agents and sort of building different types of assistants through Llama. \\r\\nI guess the two-part question is, can you just sort of talk to us a little bit about some \\r\\nof the use cases or consumer advertiser offerings you're most excited about \\r\\nenabling for some of these AI agents? And then the second one, just to sort of level \\r\\nset on timing a bit, can you just help us understand some of the larger technological \\r\\nbarriers in your mind the teams will need to overcome to really scale these types of \\r\\nagent products across the ecosystem? Thanks.\\r\\nMark Zuckerberg: Sure. So I think the main thing that I'll say on this for now is tune in to Connect \\r\\ncoming up in September. We're going to have a lot more to talk about on the \\r\\nroadmap and products that we're launching. We wanted to get the Llama 2 model \\r\\nout now. That's going to be -- that's going to underpin a lot of the new things that \\r\\nwe're building. And now we're nailing down a bunch of these additional products, \\r\\nand this is going to be stuff that we're working on for years. But I think a lot of the \\r\\njourney will kind of start later this year when we start rolling out some of these \\r\\nthings.\\r\\nBut overall, I think that there are three basic categories of products or technologies \\r\\nthat we're planning on building with generative AI. One are around different kinds \\r\\nof agents, which I'll talk about in a second. Two are just kind of generative AI\\ufffepowered features. So some of the canonical examples of that are things like in \\r\\nadvertising, helping advertisers basically run ads without needing to supply as much \\r\\ncreative or if they have an image but it doesn't fit the format, be able to fill in the \\r\\nimage for them. So I talked about that a little bit upfront in my comments. But \\r\\nthere's stuff like that across every app.\\r\\nAnd then the third category of things, I'd say, are broadly focused on productivity \\r\\nand efficiency internally. So everything from helping engineers write code faster to \\r\\nhelping people internally understand the overall knowledge base at the company \\r\\nand things like that. So there's a lot to do on each of those zones.\\n\", metadata={'page': 7, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}), -0.396153163947377), (Document(page_content='5\\r\\nOur community across the Family of Apps continues to grow. We estimate that approximately 3.07 \\r\\nbillion people used at least one of our Family of Apps on a daily basis in June, and that approximately \\r\\n3.88 billion people used at least one on a monthly basis.\\r\\nFacebook continues to grow globally and engagement remains strong. For the first time, we crossed 3 \\r\\nbillion monthly active users, with Facebook MAU ending at 3.03 billion in June, up 3% or 96 million \\r\\ncompared to last year. Facebook daily active users were 2.06 billion, up 5% or 96 million. DAUs \\r\\nrepresented approximately 68% of MAUs.\\r\\nQ2 Total Family of Apps revenue was $31.7 billion, up 12% year over year.\\r\\nQ2 Family of Apps ad revenue was $31.5 billion, up 12% or 13% on a constant currency basis.\\r\\nWithin ad revenue, the online commerce vertical was the largest contributor to year-over-year growth, \\r\\nfollowed by entertainment & media and CPG. Online commerce benefited from strong spend among \\r\\nadvertisers in China reaching customers in other markets.\\r\\nOn a user geography basis, ad revenue growth was strongest in Rest of World at 16%, followed by \\r\\nEurope, North America and Asia-Pacific at 14%, 11% and 10%, respectively. Foreign currency was a \\r\\nheadwind to advertising revenue growth in all international regions.\\r\\nIn Q2, the total number of ad impressions served across our services increased 34% and the average \\r\\nprice per ad decreased 16%. Impression growth was primarily driven by Asia-Pacific and Rest of World.\\r\\nThe year-over-year decline in pricing was driven by strong impression growth, especially from lower \\r\\nmonetizing surfaces and regions. While overall pricing remains under pressure from these factors, we \\r\\nbelieve our ongoing improvements to ad targeting and measurement are continuing to drive improved \\r\\nresults for advertisers.\\r\\nFamily of Apps other revenue was $225 million in Q2, up 3%, as strong business messaging revenue \\r\\ngrowth from our WhatsApp Business Platform was partially offset by a decline in other line items.\\r\\nWe continue to direct the majority of our investments toward the development and operation of our \\r\\nFamily of Apps. In Q2, Family of Apps expenses were $18.6 billion, representing approximately 82% of \\r\\nour overall expenses. FoA expenses were up 8% due primarily to legal-related expenses and \\r\\nrestructuring charges, partially offset by a decrease in non headcount-related operating expenses, \\r\\nincluding marketing.\\r\\nFamily of Apps operating income was $13.1 billion, representing a 41% operating margin.\\r\\nWithin our Reality Labs segment, Q2 revenue was $276 million, down 39% due to lower Quest 2 sales.\\r\\nReality Labs expenses were $4.0 billion, up 23% due to lapping a reduction in Reality Labs loss reserves \\r\\nin Q2 of last year as well as growth in employee-related costs.\\r\\nReality Labs operating loss was $3.7 billion.\\r\\nTurning now to the business outlook. There are two primary factors that drive our revenue \\r\\nperformance: Our ability to deliver engaging experiences for our community, and our effectiveness at \\r\\nmonetizing that engagement over time.\\n', metadata={'page': 4, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}), -0.3962358573417164), (Document(page_content=\"17\\r\\nfrom delays in data center projects and server deliveries. And that will just push that \\r\\nassociated CapEx, which we were planning for in '23 into '24.\\r\\nWe're still working on our '24 CapEx plans. We haven't yet finalized that, and we'll \\r\\nbe working on that through the course of this year. But I mentioned that we expect \\r\\nthat CapEx in '24 will be higher than in '23. We expect both data center spend to \\r\\ngrow in '24 as we ramp up construction on sites with the new data center \\r\\narchitecture that we announced late last year. And then we certainly also expect to \\r\\ninvest more in servers in 2024 for both AI workloads to support all of the AI work \\r\\nthat we've talked about across the core AI ranking, recommendation work along \\r\\nwith the next-gen AI efforts. And then, of course, also our non-AI workloads as we \\r\\nrefresh some of our servers and add capacity just to support continued growth \\r\\nacross the site.\\r\\nOperator: Your next question comes from the line of Youssef Squali with Truist.\\r\\nYoussef Squali: One for Mark and one for Susan maybe. Mark, you touched on this a little earlier, \\r\\nbut there is this open source versus closed source debate going on with Meta as one \\r\\nof the companies on one side of it with Llama and Llama 2. How do you see this \\r\\nmarket evolving over time? Do you see one approach as being potentially superior \\r\\nto the other, and therefore, would be likely to garner greater adoption? Or is this \\r\\nreally a win-win situation, just considering how early we are in this process and how \\r\\nlargely potential TAM is?\\r\\nAnd then Susan, your guidance for Q3 implies at the midpoint about a 20% growth, \\r\\nwhich is a pretty steep acceleration from Q2 at 11% or 12%. Could you maybe just \\r\\nhelp us understand the drivers of that acceleration? And as we look into Q4, the \\r\\ncomps remain pretty easy. Is that sustainable to year-end?\\r\\nMark Zuckerberg: Yes, I can speak briefly to the first one. I do think that there will continue to be both \\r\\nopen and closed AI models. I think there are a bunch of reasons for this. There are \\r\\nobviously a lot of companies that their business model is to build a model and then \\r\\nsell access to it. So for them, making it open would undermine their business model. \\r\\nThat is not our business model. We want to have the -- like we view the model that \\r\\nwe're building as sort of the foundation for building products. So if by sharing it, we \\r\\ncan improve the quality of the model and improve the quality of the team that we \\r\\nhave that is working on that, that's a win for our business of basically building better \\r\\nproducts.\\r\\nSo I think you'll see both of those models. There are also some important safety \\r\\nquestions that I think we'll need to continue thinking about over time. There are a \\r\\nnumber of people who are out there saying that once the AI models get past a \\r\\ncertain level of capability, it can become dangerous for them to become just in the \\r\\nhands of everyone openly. I think -- what I think is pretty clear is that we're not at \\r\\nthat point today. I think that there's consensus generally among people who are \\r\\nworking on this in the industry and policy folks that we're not at that point today. \\r\\nAnd it's not exactly clear at what point you reach that.\\n\", metadata={'page': 16, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}), -0.4270880918396964), (Document(page_content=\"13\\r\\n-- I've thought for a while that the opportunity around public conversations and kind \\r\\nof a text-based product was bigger than what had been executed yet in the market. \\r\\nSo we had a relatively small team work on that. And this year, I think there have \\r\\nbeen two things that have been -- that have just vastly exceeded my expectations. \\r\\nOne was Llama, the initial model, we thought it was interesting, but the scale of \\r\\nadoption, even just for the research version really spurred us to go do Llama 2, and \\r\\nthat was vastly more than we expected.\\r\\nAnd the second is Threads has been dramatically more than we expected in terms of \\r\\nthe adoption and the rate of that. We thought this was going to be kind of a project \\r\\nthat we just -- we had a small team working on for a while, but it really kind of blew \\r\\nup and created a big opportunity immediately. But no, I mean, I think over time, you \\r\\nshould expect that we're going to focus on AI and the metaverse. AI includes both \\r\\nall of the ranking and recommendation systems that power the core apps, so all the \\r\\ncontent that you're seeing in Facebook and Instagram, all the ads content that \\r\\nyou're seeing. It also underpins all the safety systems that we build.\\r\\nAnd increasingly, it's all the generative AI stuff, so all the agents, all the generative \\r\\nfeatures that we're going to be rolling out and a lot of the other work that's going to \\r\\nunderly some of the efficiency stuff that we're doing internally. And then the \\r\\nmetaverse stuff, I think we've talked about for a while. So I don't think that there's \\r\\nmuch change there, except that I'd say the signals that we're getting from the \\r\\nmarket are it's certainly not getting adopted a lot faster than we expected so that's \\r\\nsort of the somewhat sobering signal.\\r\\nBut on the other side, I think a lot of companies that otherwise are doing, we \\r\\nrespect and do great work, we don't necessarily view as building things that are \\r\\nahead of where we are, which gives us confidence that we think that the long-term \\r\\nthesis still will hold there. We think that we're going to be the leaders in it. And \\r\\nnothing that we're seeing from the market makes us rethink that in a fundamental \\r\\nway. So I think we're going to continue focusing on AI and the metaverse as the two\\r\\nbig thrusts of what we're doing and all the other things kind of fall out of that.\\r\\nSusan Li: And Mark, I'll take the second question that you asked around the Q2 revenue \\r\\nacceleration. So I think there were a couple of parts. One was around the \\r\\nacceleration in the core ads business. The second was on the Advantage+ products. \\r\\nIn terms of the Q2 revenue acceleration, I'd highlight there are a few factors driving \\r\\nthat. The first is, frankly, we're lapping a weaker demand period, including the first \\r\\nfull quarter of the war in Ukraine and the suspension of our services in Russia.\\r\\nSecond, we saw increased supply and improvements to ad performance, including \\r\\nimproved Reels monetization as we continue to work down the Reels revenue \\r\\nheadwind. And third, there were lower FX headwinds for us this quarter. So those \\r\\nwere all three things that helped drive the revenue acceleration in Q2.\\r\\nIn terms of the question about Advantage+ specifically, Advantage+ is one of \\r\\nmultiple AI-powered ad products that we have right now in the market. With \\r\\nAdvantage+ specifically, we're seeing strong adoption and particular success with \\n\", metadata={'page': 12, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}), -0.43081131466587674), (Document(page_content=\"3\\r\\nWhile we're on messaging, I'll mention that we started rolling out Channels on WhatsApp last month. \\r\\nIt's a simple, reliable and private way to receive important updates from people and organizations, and \\r\\nI'm quite excited for more people to try it as we bring the product to more countries through the rest of \\r\\nthis year.\\r\\nBeyond the recommendations and ranking systems across our products, we're also building leading \\r\\nfoundation models to support a new generation of AI products.\\r\\nWe've partnered with Microsoft to open source Llama 2, the latest version of our large language model, \\r\\nand make it available for both research and commercial use. We have a long history of open-sourcing \\r\\nour infrastructure and AI work -- from PyTorch, which is the leading machine learning framework, to\\r\\nmodels like Segment Anything, ImageBind, and Dino, to basic infrastructure as part of the Open \\r\\nCompute Project. We've found that open sourcing our work allows the industry, including us, to benefit \\r\\nfrom innovations that come from everywhere. And these are often improvements in safety and security, \\r\\nsince open source software is more scrutinized and more people can find and identify fixes for issues. \\r\\nThe improvements also often come in the form of efficiency gains, which should hopefully allow us and \\r\\nothers to run these models with less infrastructure investment going forward. I'm really looking forward \\r\\nto seeing the improvements that the community makes to Llama 2.\\r\\nWe're also building a number of new products ourselves using Llama that will work across our services. \\r\\nI'm going to share more details later this year, but you can imagine lots of ways AI could help people \\r\\nconnect and express themselves in our apps: creative tools that make it easier and more fun to share \\r\\ncontent, agents that act as assistants, coaches, or that can help you interact with businesses and \\r\\ncreators, and more. These new products will improve everything that we do across both mobile apps \\r\\nand the metaverse -- helping people create worlds and the avatars and objects that inhabit them as \\r\\nwell.\\r\\nAs our investments in AI continue, we remain fully committed to the metaverse vision as well. We've \\r\\nbeen working on both of these two major priorities for many years in parallel now, and in many ways \\r\\nthe two areas are overlapping and complementary.\\r\\nThe next big thing on the Reality Labs side is the launch of our Quest 3 mixed reality headset at Connect. \\r\\nIt's our most powerful headset yet -- with better displays and resolution, and next gen Qualcomm \\r\\nchipset with 2x the graphics performance. It will also have the best immersive content library out there, \\r\\nand it's 40% thinner than Quest 2. Its mixed reality seamlessly blends your physical world and the virtual \\r\\none by intelligently understanding the physical space around you. We pioneered mixed reality with our \\r\\nQuest Pro headset, and Quest 3 takes that to the next level. Others in the industry are of course working \\r\\non bringing mixed reality to the market too, but Quest 3 is going to be the first mainstream, accessible \\r\\ndevice that we expect many millions of people will get to experience this technology with.\\r\\nThe metaverse content and software vision continues coming together as well. We recently announced \\r\\nthat Roblox is coming to Quest with an open beta on App Lab. For Horizon, the team is focused on \\r\\nretention right now and we're making good progress on that. We've made big improvements on avatars \\r\\nas well, and that’s going to be a bridge between our mobile apps and our VR and mixed reality \\r\\nexperiences. We'll have a lot more to share on both our metaverse and AI work at our Connect\\r\\nconference, which we'll be hosting in Hacker Square at our headquarters on September 27th. It's going\\r\\nto be a good one, so tune in!\\n\", metadata={'page': 2, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}), -0.43374072888026216), (Document(page_content=\"14\\r\\nthe e-commerce and retail verticals, and we've seen good traction with other \\r\\nverticals like CPG, especially DTC brands, and we're continuing to launch features to \\r\\nunlock use cases for advertisers and make it easier for them to adopt Advantage+ \\r\\ncampaigns and measure their performance gains. So we've got a lot in the pipeline \\r\\nthere that we're excited about for both the Advantage+ Shopping Campaigns \\r\\nspecifically, which were our first sort of foray into this area, but then the \\r\\nAdvantage+ portfolio more broadly that basically enables us to take that same \\r\\nplaybook of helping advertisers iterate and test very quickly and apply it to many \\r\\ndifferent steps of the end-to-end ad buying experience.\\r\\nSo the feedback and results that we've seen from advertisers is good, and we think \\r\\nit's a really promising area that we're continuing to invest in. But it's one of many \\r\\nways that we're using AI to continue to sort of help make our ads systems and \\r\\nrecommendations and ranking engines, more performant to deliver better \\r\\nmeasurement and results to advertisers.\\r\\nOperator: Your next question comes from the line of Justin Post with Bank of America Merrill \\r\\nLynch.\\r\\nJustin Post: I guess I want to follow up on Reality Labs, passing $40 billion in losses and \\r\\nincreasing annually next year. Just think about -- maybe help us understand how the \\r\\nROI on the business, how you're thinking about that investment, either on a stand\\ufffealone basis or as a complement to the Family of Apps, if you're thinking about it \\r\\nfrom an investor perspective.\\r\\nMark Zuckerberg: Well, I think it's both over time. The primary way that I think about this is that we've \\r\\nbeen able to show that we can -- we've been quite successful at building large-scale \\r\\nsocial experiences within the constraints of platforms that often our competitors are \\r\\ndefining. I think we're going to be able to do even better work. And there's a lot of \\r\\nthings that I would like to see us build that we just can't because we're -- of the \\r\\nways that we're constrained by the competitors who build these platforms.\\r\\nAnd over time, the main way that I think about this is from a product and mission \\r\\nperspective is we're here to build awesome experiences that help people connect. I \\r\\nthink helping to shape the next platform is going to unlock that in a profound way \\r\\nfor decades to come. And that's what I'm here to do and I think what a lot of people \\r\\nare here to do. So that's kind of the first principles part of this.\\r\\nFrom a business perspective, I think that there's going to end up being a large \\r\\nbusiness component of this that is Reality Labs-specific. Directly, the products that \\r\\nwe're building there and having that be a good business. But I also think that a lot of \\r\\nthis is going to be that it unlocks a lot of value for the other experiences, the current \\r\\napps that we have, what you think of as the Family of Apps, where we currently, just \\r\\na lot of the potential value, whether it's just engagement that could be created, \\r\\nfeatures that we would love to build that we're not allowed to because Apple or \\r\\nothers just don't allow us to build those things. And I think that that's really \\r\\nunfortunate for the industry.\\n\", metadata={'page': 13, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}), -0.4653879063565991), (Document(page_content=\"10\\r\\nEric Sheridan: Maybe one for Mark, one for Susan. Mark, just following up on Brian's question. I \\r\\ndid want to ask just to draw that out a little bit more, how you think about the \\r\\nextensions of the developer community sort of growing up around a platform like \\r\\nLlama 2. And we were intrigued by the Microsoft announcement. You traditionally \\r\\nhave been more of a consumer-facing company with product. Could this provide \\r\\nyou an avenue to be more of an enterprise-facing company over the long term? And \\r\\nis there a strategy there that maybe we haven't seen from you in the past? I'd love \\r\\nto flesh that out a little bit.\\r\\nAnd Susan, just one for you. The RL losses just continue to build. And I think we \\r\\ncontinue to struggle a little bit of what drivers of those losses are and how should \\r\\nwe think about some of the components driving the losses versus elements of \\r\\nearning a return on those losses over the medium to long term as Mark sort of \\r\\nframed the time frame around metaverse. So any color there about sort of rate of \\r\\nchange or components of the losses in RL, I think, would be super helpful.\\r\\nMark Zuckerberg: Yes, sure. I'll take a cut at both of those, and Susan can jump into the second one if \\r\\nthere's anything that she wants to add there as well. All right. So Llama is an open \\r\\nsource project, which is a little bit different from building out a developer platform, \\r\\nalthough there will be an ecosystem around this. What we've seen around open \\r\\nsource work that we've done, which we've done a lot of in our core infrastructure \\r\\nwork, design of servers and data centers and basic infrastructure as well as in AI. \\r\\nAnd I pointed out some of these in my remarks upfront, like PyTorch and just a \\r\\nbunch of other models that we've released recently.\\r\\nOne of the things that we've seen is that when you release these projects publicly \\r\\nand open source, there tend to be a few categories of innovations that the \\r\\ncommunity makes. So on the one hand, I think it's just good to get the community \\r\\nstandardized on the work that we're doing. That helps with recruiting because a lot \\r\\nof the best people want to come and work at the place that is building the things \\r\\nthat everyone else uses. It makes sense that people are used to these tools from \\r\\nwherever else they're working. They can come here and build here.\\r\\nBut in terms of improvements that we expect to see from the community, there are \\r\\nreally a few different types. One is specifically around safety and security. That's a\\r\\nvery important issue in AI. In open source software overall, we've just seen through \\r\\nthe history here that open-source software gets scrutinized more and therefore \\r\\nends up being more secure and safer. And we think that there's a very good chance \\r\\nthat's the likely outcome here. And whatever improvements that people make to \\r\\nharden it in the community, we'll be able to roll that into our work, both for the \\r\\nfirst-party products that we'll launch as well as making it easy to propagate that \\r\\nacross the industry and make the AI that everyone uses safer.\\r\\nI would also hope to see efficiency improvements. I mean, even from the initial \\r\\nLlama that was just released as a research project, some of the improvements are \\r\\naround things like quantization and being able to run the model way more \\r\\nefficiently. Now we're spending so much on AI CapEx that all the help that we can \\r\\nget from the community to make this stuff more efficient to run, will be helpful not \\n\", metadata={'page': 9, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}), -0.47782168123815016), (Document(page_content=\"6\\r\\nOn the first - overall engagement within Facebook and Instagram remains strong. Reels continues to \\r\\ngrow and drive incremental engagement. On Facebook Feed in particular, recommended content from \\r\\naccounts you don't follow has increased significantly over the past year while also becoming more \\r\\nincremental to engagement - demonstrating that people are getting added value from discovering \\r\\ncontent from unconnected accounts. Looking forward, we are optimistic about our ability to increase \\r\\nthat value even further by leveraging advanced AI techniques to improve recommendations.\\r\\nIn addition to improving the value people get within our family of apps today, we’re also investing in \\r\\nentirely new experiences for the future. We’re standing up infrastructure to support new AI-powered \\r\\nproducts across our services, which will give people more tools to express themselves and connect. And \\r\\nwe’ve been pleased with the initial reception of our new standalone app, Threads, since its launch \\r\\nearlier this month. Our focus now is on further developing this into a product that will be valuable for a \\r\\nlarge set of people over time.\\r\\nMoving to the other driver of revenue - improving monetization. Here, we’re focused on improving \\r\\nmonetization efficiency of products that monetize at lower rates today, like Reels and our messaging \\r\\nservices; and, more broadly, driving measurable performance and returns for our advertisers.\\r\\nOn Reels, we are making good progress on monetization, with more than 3/4 of our advertisers now \\r\\nusing Reels ads. We remain focused on further reducing the Reels revenue headwind and narrowing the \\r\\nmonetization efficiency gap with our more mature surfaces. However, we continue to expect time on \\r\\nReels will monetize at a lower rate than Stories and Feed for the foreseeable future since people scroll \\r\\nmore slowly through video content.\\r\\nWithin messaging, billions of people and millions of businesses use our messaging services every day to \\r\\nconnect. We see a significant opportunity to build tools and functionality for businesses to help facilitate \\r\\nthose interactions and are seeing early but promising progress with WhatsApp’s paid messaging solution \\r\\ntoday.\\r\\nIn terms of our work to drive measurable performance for advertisers - it’s concentrated in two primary \\r\\nareas: AI and onsite conversions.\\r\\n• We’re leveraging AI to move our systems towards using fewer, larger models that enable us to \\r\\nleverage learnings across product surfaces and deploy improvements more quickly, broadly, and \\r\\nefficiently. We’re also leveraging AI to power advanced ads products, like Advantage+ shopping, \\r\\nwhich continues to gain adoption. We’re seeing this work translate into results for advertisers as \\r\\nconversion growth remained strong in Q2.\\r\\n• In terms of driving onsite conversions, we continue to see strong results with Click-to-messaging \\r\\nads and are well positioned given our suite of messaging applications. Daily Click-to-WhatsApp \\r\\nads revenue continues to grow very quickly at over 80% year-over-year. We also recently started \\r\\ntesting the ability to buy Click-to-WhatsApp ads directly from the WhatsApp Business App, \\r\\nwhich now has more than 200 million monthly users. Looking ahead, we’re focused on enabling \\r\\nbusinesses to optimize for conversions further down the funnel in our messaging applications. \\r\\nWe are also investing in scaling other onsite objectives like lead generation and Shops ads.\\r\\nBefore turning to our revenue outlook, I’d also like to talk about our investment philosophy. We expect \\r\\nto bring the discipline and habits that we built during this year of efficiency with us as we plan for the \\r\\nfuture. At the same time, we remain focused on investing in the significant opportunities ahead. Part of \\n\", metadata={'page': 5, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}), -0.48652607265663383), (Document(page_content=\"19\\r\\nthat new products, it sounds like are going to be really central to instructing that \\r\\nnumber. Just curious how dependent it also is based on just views of revenue \\r\\ngrowth in '24 and the relationship there.\\r\\nSusan Li: Yes. Thank you, Brad. I can go ahead and take that one. So our growth in AI \\r\\ninvestments is really the thing that is driving the growth in our 2024 CapEx outlook. \\r\\nAnd I think there are a couple of components to that. There is both the core AI \\r\\nwork, which powers our ranking and recommendation systems, which underpins \\r\\nboth a lot of our content ranking and engagement growth as well as the \\r\\nmonetization work. And that's an area where we're able to measure the ROI of our \\r\\ninvestments there, and we feel good about the ROI of those investments. And we \\r\\nwant to continue investing appropriately to drive revenue growth, at the same time \\r\\nbeing mindful of our desire to, over the long term, decrease capital intensity.\\r\\nThere's also another component, which is the next-generation AI efforts that we've \\r\\ntalked about around advanced research and gen AI, and that's a place where we're \\r\\nalready standing up training clusters and inference capacity. But we don't know \\r\\nexactly what we'll need in 2024 since we don't have any at-scale deployments yet of \\r\\nconsumer business-facing features. And the scale of the adoption of those products \\r\\nis ultimately going to inform how much capacity we need.\\r\\nWe think these are both going to be compelling investment opportunities and some \\r\\nof the AI capacity is fungible, so if we don't need -- end up needing some of the \\r\\ncapacity for our gen AI work, we'll be able to allocate it to our core AI work, \\r\\nsupporting ads and engagement. But we're really still working on our '24 plans. We \\r\\nwill have a clearer and more quantitative outlook as those plans shape up. But we \\r\\nare mindful of our intention to reduce the capital intensity of these investments \\r\\nover time.\\r\\nKenneth Dorell: Great. Thank you for joining us today. We appreciate your time. And we look \\r\\nforward to speaking again soon.\\r\\nOperator: And this concludes today's conference call. Thank you for joining us. You may now \\r\\ndisconnect your lines.\\n\", metadata={'page': 18, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}), -0.488194952907429), (Document(page_content=\"18\\r\\nSo I think there are people who are kind of making that argument in good faith, who \\r\\nare actually concerned about the safety risk. I think that there are probably some \\r\\nbusinesses that are out there making that argument because they want it to be \\r\\nmore closed, because that's their business so I think we need to be wary of that. But \\r\\nI think for both of those reasons, the business reasons and the safety reasons, I \\r\\nthink there will continue to be a mix of open and closed models, and it's not just \\r\\ngoing to be like one thing is what everyone uses. I think different businesses will use \\r\\ndifferent things for different reasons.\\r\\nAnd by open sourcing Llama now, we're not -- on the second question around \\r\\nsafety, we intentionally did not go out there and say we're going to open source \\r\\nevery single thing in the future because we do want to have the space to be able to \\r\\nlook at how the safety landscape evolves. And if we think that we do cross some \\r\\nkind of critical threshold in the future, it may not be the right thing to open source it \\r\\nin the future. But for our business model at least, since we're not selling access to \\r\\nthis stuff, it's a lot easier for us to share this with the community because it just \\r\\nmakes our products better and other people’s, and that, I think, is a really healthy \\r\\ndynamic for the industry.\\r\\nSusan Li: And thanks, Youssef, for your second question, which I'll take. You asked about our \\r\\nQ3 revenue outlook and maybe also what that implies looking forward into Q4. So \\r\\non the further acceleration that we've guided to in Q3, first of all, I'd just point out \\r\\nthat Q3'22 revenue declined 4.5% year-over-year so we're really lapping a much \\r\\nweaker demand period a year ago, and we've certainly seen demand this year \\r\\nstabilize and so it's really a much easier compare.\\r\\nWe also are expecting that currency is going to flip to a 3-point tailwind from a 1-\\r\\npoint headwind last quarter. And finally, as I've mentioned earlier on this call, we've \\r\\nbeen investing for a long time and I think executing really well across our core \\r\\nmonetization work and continuing to grow engagement. And I think we're going to \\r\\nbenefit from all of the investments that we've made in those historical and key \\r\\npriorities for us.\\r\\nIn terms of what this means for Q4, we're not sharing a Q4 revenue outlook yet. \\r\\nThere are obviously some tailwinds to year-over-year growth in Q4. Again, the same \\r\\npoint about a weaker compare applies. And at current rates, FX would be a larger \\r\\ntailwind in Q4 than we're expecting it to be in Q3. But we've had sizable fluctuations \\r\\nin advertiser demand over the last year. It's been a pretty volatile period. So while \\r\\nwe're seeing strong advertiser demand now and that's certainly informing our \\r\\noutlook, it's harder to predict as we look further forward, and there are a wide \\r\\nrange of possible outcomes in Q4.\\r\\nKenneth Dorell: Dave, we have time for one last question.\\r\\nOperator: That will come from the line of Brad Erickson with RBC Capital Markets.\\r\\nBradley Erickson: Just had a follow-up, I guess, on the CapEx. Just in that planning process you keep \\r\\nalluding to for the '24 CapEx outlook, I think a lot of your commentary here implies \\n\", metadata={'page': 17, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}), -0.4938289530694018), (Document(page_content=\"2\\r\\nexamples of how our leaner organization and some of the cultural changes that we've made can build \\r\\nhigher quality products faster, and this is probably the biggest example so far. The Year of Efficiency was \\r\\nalways about two different goals: becoming an even stronger technology company, and improving our \\r\\nfinancial results so we can invest aggressively in our ambitious long term roadmap. Now that we've \\r\\ngotten through the major layoffs, the rest of 2023 will be about creating stability for employees, \\r\\nremoving barriers that slow us down, introducing new AI-powered tools to speed us up, and so on.\\r\\nOver the next few months, we're going to start planning for 2024, and I'm going to be focused on \\r\\ncontinuing to run the company as lean as possible for these cultural reasons even though our financial \\r\\nresults have improved. I expect that we’re still going to hire in key areas, but newly budgeted headcount \\r\\ngrowth is going to be relatively low. That said, as part of this year's layoffs, many teams chose to let \\r\\npeople go in order to hire different people with different skills they need, so much of that hiring is going \\r\\nto spill into 2024.\\r\\nThe other major budget point that we're working through is what the right level of AI capex is to support \\r\\nour roadmap. Since we don't know how quickly our new AI products will grow, we may not have a clear \\r\\nhandle on this until later in the year.\\r\\nMoving onto our product roadmap, I've said on a number of these calls that the two technological \\r\\nwaves that we're riding are AI in the near term and the metaverse over the longer term.\\r\\nInvestments that we've made over the years in AI, including the billions of dollars we've spent on AI \\r\\ninfrastructure, are clearly paying off across our ranking and recommendation systems and improving \\r\\nengagement and monetization.\\r\\nAI-recommended content from accounts you don't follow is now the fastest growing category of content \\r\\non Facebook's feed. Since introducing these recommendations, they have driven a 7% increase in overall \\r\\ntime spent on the platform. This improves the experience because you can now discover things that you \\r\\nmight not have otherwise followed or come across. Reels is a key part of this Discovery Engine, and \\r\\nReels plays exceed 200 billion per day across Facebook and Instagram. We're seeing good progress on \\r\\nReels monetization as well, with the annual revenue run-rate across our apps now exceeding $10 billion, \\r\\nup from $3 billion last fall.\\r\\nBeyond Reels, AI is driving results across our monetization tools through our automated ads products, \\r\\nwhich we call Meta Advantage. Almost all our advertisers are using at least one of our AI-driven \\r\\nproducts. We've also deployed Meta Lattice, a new model architecture that learns to predict an ad's \\r\\nperformance across a variety of datasets and optimization goals. And we introduced AI Sandbox, a \\r\\ntesting playground for generative AI-powered tools like automatic text variation, background \\r\\ngeneration, and image outcropping.\\r\\nBusiness messaging is another key piece of our monetization strategy, and we recently announced that \\r\\nthe 200 million users of our WhatsApp Business app will now be able to create Click-to-WhatsApp ads \\r\\nfor Facebook and Instagram without needing a Facebook account. This is a pretty big unlock, particularly \\r\\nin countries where WhatsApp is often the first step to bringing a business online. Paid messaging is a bit \\r\\nearlier but is also showing good adoption. The number of businesses using our paid messaging products \\r\\nhas doubled year over year.\\n\", metadata={'page': 1, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}), -0.4951313032480642), (Document(page_content=\"16\\r\\nOr it could just be that this is such an idiosyncratic case because of all of the factors \\r\\nthat are happening around Twitter or X, I guess, it's called now. So it's hard to say. I \\r\\nmean, but I think when something works or doesn't, you can often point to the \\r\\nreason why it did or didn't. And I think it is an interesting intellectual question of \\r\\nwhether you could have known that a priori, that, that was actually going to be the \\r\\ncase.\\r\\nSo I'm not sure. But also rather than trying to kind of analyze that, I'd say we have a \\r\\nlot of work to do to really make Threads reach its full potential. That's not a \\r\\nforegone conclusion yet even though I think we're off to a great start, and I'm \\r\\noptimistic that over time, this could be a fifth great app in the Family of Apps. But \\r\\nwe have a lot of basic work to do. \\r\\nAnd we have a basic playbook here, which is build an experience. It's got to be \\r\\nsomething that people like so it has to have product market fit. Once you get that, \\r\\nit's not always retentive. So a lot of people might like an experience but you need to \\r\\nkind of tune it so that way the numbers works so that people who use it are \\r\\ncontinuing. We feel like we're getting to a good place on that with Threads. There's \\r\\nstill a lot of basic functionality to build.\\r\\nOnce we feel like we're in a very good place on that, then I'm highly confident that \\r\\nwe're going to be able to pour enough gasoline on this to help it grow once we get \\r\\nto the point where we feel good that everyone who's using it is going to continue \\r\\nusing it at a high rate. And then a few years, once we get to the point where it's at \\r\\nhundreds of millions of people, assuming we can get there, then we'll worry about \\r\\nmonetization. But I mean, that's basically the playbook that we're focused on.\\r\\nAnd so rather than thinking about right now, like what does this mean for other \\r\\nthings like it that we can build, I'd say we're really just focused on taking this \\r\\nopportunity, which is an awesome one that we didn't expect to this scale and \\r\\nmaking sure we make the most of this and execute it. But I do think it has been sort \\r\\nof this weird anomalous thing in the tech industry that there hasn't been an app for \\r\\npublic discussions like this that has reached 1 billion people.\\r\\nWhen I look at all the different social experiences, it just seems like there should be \\r\\none like this. I think there are a lot of reasons that you can point to why that might \\r\\nhave not been the case historically. But it's awesome that we get a chance to work \\r\\non this and I'm really optimistic about where we are. But it's going to be a long road \\r\\nahead.\\r\\nSusan Li: And Doug, on your second question about our 2023 CapEx forecast and the impact \\r\\non '24, so I'll start with 2023. The reduced forecast that we gave for '23, I mentioned \\r\\non the call, is driven both by some cost savings, particularly on non-AI servers, \\r\\nwhere were -- we previously had some underutilized capacity, and we've been \\r\\nidentifying ways to be more efficient in the way that we allocate that capacity \\r\\ntowards all of our various needs, as well as shifts in CapEx into 2024 that's coming \\n\", metadata={'page': 15, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}), -0.5181839455955206), (Document(page_content=\"11\\r\\njust for individuals to be able to run powerful models on their laptop or locally or \\r\\nwithout a huge amount of compute, so individuals can afford it. But that should \\r\\nhopefully translate over time into just a more efficient infrastructure for us, which \\r\\nhopefully could be quite a big savings. So that's more what we're seeing there.\\r\\nWe partnered with Microsoft specifically because we don't have a public cloud \\r\\noffering. So this isn't about us getting into that. It's actually the opposite. We want \\r\\nto work with them because they have that and others have that, and that was the \\r\\nthing that we aren't planning on building out. But one of the things that you might \\r\\nhave noticed is in addition to making this open through the open source license, we \\r\\ndid include a term that for the largest companies, specifically ones that are going to \\r\\nhave public cloud offerings, that they don't just get a free license to use this. They'll \\r\\nneed to come and make a business arrangement with us.\\r\\nAnd our intent there is we want everyone to be using this. We want this to be open. \\r\\nBut if you're someone like Microsoft or Amazon or Google and you're going to \\r\\nbasically be reselling these services, that's something that we think we should get \\r\\nsome portion of the revenue for. So those are the deals that we intend to be \\r\\nmaking, and we've started doing that a little bit. I don't think that that's going to be \\r\\na large amount of revenue in the near term. But over the long term, hopefully, that \\r\\ncan be something.\\r\\nNot sure if there's anything else to add on that, but Susan, you can jump in if you \\r\\nwant. On RL, let me just jump over to that for a second. I don't think we're going to \\r\\nbreak out numbers right now but I'll defer to Susan on that. I mean, one big thing \\r\\nfor the next year is the launch of Quest 3, right? And basically, this is going to be the \\r\\nbiggest headset that we've released since 2020 when we came out with Quest 2. \\r\\nAnd there are just a lot of expenses related to bringing that to market. And I think \\r\\nthat's, at least in the near term, going to be one of the major drivers of this.\\r\\nSo I think that's probably the main thing that I would point to there. We're \\r\\ncontinuing to -- I continue to think, looking at the VR work that we're doing and the \\r\\nAR work, some of the neural interfaces work, I think we're leading in these areas. It's \\r\\nbeen good to see what others in the industry have done because to some degree, it \\r\\ngives us more confidence that we're on the right track.\\r\\nAnd even though I know from an investor standpoint, most people aren't investing \\r\\non quite as long of a time horizon as we are here, so I kind of get that, a lot of \\r\\ninvestors might want to see us spending less here in the near term. My view is that \\r\\nwe are leading in these areas. I believe that they're going to be big over time. I think \\r\\nwe've shown that we can deliver good business results in the near term while \\r\\ninvesting ambitiously in the long term. So I'm planning on continuing to do that, and \\r\\nI do continue to believe that over time, we will be happy that we did that. So I don't \\r\\nknow if Susan, if there's anything you wanted to add there.\\r\\nSusan Li: Sure. Let me just add a couple of things. Very quickly on the first question, I just \\r\\nwant to clarify that we don't expect that Llama is going to result in a separate \\r\\ntopline enterprise revenue line, so just making sure we're totally clear.\\n\", metadata={'page': 10, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}), -0.5195032772863446), (Document(page_content=\"9\\r\\nFor AI agents specifically, I guess what I'd say is one of the things that's different \\r\\nabout how we think about this compared to some others in the industry is we don't \\r\\nthink that there's going to be one single AI that people interact with, just because \\r\\nthere are all these different entities on a day-to-day basis that people come across, \\r\\nwhether they're different creators or different businesses or different apps or things \\r\\nthat you use. So I think that there are going to be a handful of things that are just \\r\\nsort of focused on helping people connect around expression and creativity and \\r\\nfacilitating connections.\\r\\nI think there are going to be a handful of experiences around helping people \\r\\nconnect with the creators who they care about and helping creators foster their \\r\\ncommunities. And then the one that I think is going to have the fastest direct \\r\\nbusiness loop is going to be around helping people interact with businesses. You can \\r\\nimagine a world on this where over time, every business has as an AI agent that \\r\\nbasically people can message and interact with. And it's going to take some time to \\r\\nget there, right? I mean, this is going to be a long road to build that out.\\r\\nBut I think that, that's going to improve a lot of the interactions that people have \\r\\nwith businesses as well as if that does work, it should alleviate one of the biggest \\r\\nissues that we're currently having around messaging monetization is that in order to \\r\\n-- for a person to interact with a business, it's quite human labor-intensive for a \\r\\nperson to be on the other side of that interaction, which is one of the reasons why \\r\\nwe've seen this take off in some countries where the cost of labor is relatively low.\\r\\nBut you can imagine in a world where every business has an AI agent, that we can \\r\\nsee the kind of success that we're seeing in Thailand or Vietnam with business \\r\\nmessaging could kind of spread everywhere. And I think that's quite exciting. But \\r\\noverall, I think we're going to follow a playbook that's similar to what we normally \\r\\ndo on products. \\r\\nIn terms of how quickly some of these new products scale, that's one of the big \\r\\nunknowns for the business and one of the things that we're debating heavily when \\r\\nthinking through the amount of AI CapEx to bring online because the reality is we \\r\\njust don't know how quickly these will scale. And we want to have the capacity in \\r\\nplace in case they scale very quickly. But because they're kind of brand-new things, \\r\\nthere aren't that many precedents for things like this. It's actually quite hard to \\r\\nforecast.\\r\\nSo I don't know, Susan, if there's anything on that last point that you want to jump \\r\\nin on. But I think that's probably all I'll say on this for now. But I'm really excited \\r\\nabout the segment. This is just going to be -- it fits into all the different products \\r\\nthat we're building really well. I think that this is going to both complement and \\r\\ntouch and transform every single thing that we're doing, and I'm really excited for it. \\r\\nSo tune in to Connect.\\r\\nOperator: Your next question comes from the line of Eric Sheridan with Goldman Sachs.\\n\", metadata={'page': 8, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}), -0.5352318938131391), (Document(page_content=\"1\\r\\nMeta Platforms, Inc. (META)\\r\\nSecond Quarter 2023 Results Conference Call\\r\\nJuly 26th, 2023\\r\\nKen Dorell, Director, Investor Relations\\r\\nThank you. Good afternoon and welcome to Meta Platforms second quarter 2023 earnings conference \\r\\ncall. Joining me today to discuss our results are Mark Zuckerberg, CEO and Susan Li, CFO.\\r\\nBefore we get started, I would like to take this opportunity to remind you that our remarks today will \\r\\ninclude forward‐looking statements. Actual results may differ materially from those contemplated by \\r\\nthese forward‐looking statements.\\r\\nFactors that could cause these results to differ materially are set forth in today’s press release, and in \\r\\nour quarterly report on form 10-Q filed with the SEC. Any forward‐looking statements that we make on \\r\\nthis call are based on assumptions as of today and we undertake no obligation to update these \\r\\nstatements as a result of new information or future events.\\r\\nDuring this call we will present both GAAP and certain non‐GAAP financial measures. A reconciliation of \\r\\nGAAP to non‐GAAP measures is included in today’s earnings press release. The press release and an \\r\\naccompanying investor presentation are available on our website at investor.fb.com.\\r\\nAnd now, I’d like to turn the call over to Mark.\\r\\nMark Zuckerberg, CEO\\r\\nThanks Ken, and thanks everyone for joining.\\r\\nThis was a good quarter for our business. We're seeing strong engagement trends across our apps. \\r\\nThere are now more than 3.8 billion people who use at least one of our apps every month. Facebook \\r\\nnow has more than 3 billion monthly actives -- with daily actives continuing to grow around the world, \\r\\nincluding in the US and Canada.\\r\\nIn addition to our core products performing well, I think we have the most exciting roadmap ahead that \\r\\nI've seen in a while. We've got continued progress on Threads, Reels, Llama 2, and some ground\\ufffebreaking AI products in the pipeline as well as the Quest 3 launch coming up this fall. We're heads down \\r\\nexecuting on all of this right now, and it's really good to see the decisions and investments that we've \\r\\nmade start to play out.\\r\\nOn Threads, briefly, I'm quite optimistic about our trajectory. We saw unprecedented growth out of the \\r\\ngate and more importantly we're seeing more people coming back daily than I'd expected. And now, \\r\\nwe're focused on retention and improving the basics. And then after that, we'll focus on growing the \\r\\ncommunity to the scale we think is possible. Only after that will we work on monetization. We've run \\r\\nthis playbook many times before -- with Facebook, Instagram, WhatsApp, Stories, Reels, and more -- and \\r\\nthis is as good of a start as we could have hoped for, so I'm really happy with the path we're on here. \\r\\nOne note that I want to mention about the Threads launch related to our Year of Efficiency is that the \\r\\nproduct was built by a relatively small team on a tight timeline. We've already seen a number of \\n\", metadata={'page': 0, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}), -0.5364315177987284), (Document(page_content=\"15\\r\\nI think that there's a lot of lost engagement that would have happened, a lot of the \\r\\nmonetization and value that gets created. I mean, just look at what happened with \\r\\nApp Tracking Transparency, the massive value destruction for small businesses \\r\\nbecause of the rules that were set by another platform. And that's not -- in a future \\r\\nversion of the world, it's not -- that would be recognized not by device sales or some \\r\\nnew, like, experience that we're building in Reality Labs. You would basically see \\r\\nthat accrue through more efficient monetization of the apps and experiences that \\r\\nwe're building in other places. So I think it's going to be both.\\r\\nBut I think this is just a very fundamental thing. This is a very long-term bet. At a \\r\\ndeep level, I understand the discomfort that a lot of investors have with it because \\r\\nit's just outside of the model of, I think, even most long-term investors, how you \\r\\nwould think about this. And look, I mean, I can't guarantee you that I'm going to be \\r\\nright about this bet. I do think that this is the direction that the world is going in. \\r\\nThere are 1 billion or 2 billion people who have glasses today. I think in the future, \\r\\nthey're all going to be smart glasses.\\r\\nAnd all the time that we spend on TVs and computers, I think that's going to get \\r\\nmore immersive and look something more like VR in the future. And I think that \\r\\nwhat we're seeing is richer ways for people to communicate across even the mobile \\r\\napps that we have going from text to photos to videos, just a continual trend \\r\\ntowards being more immersive. So like all of these trends line up to make me think \\r\\nthat this is the right thing.\\r\\nI think we're going to be happy that we did this. So that's kind of how I think about \\r\\nit, both from kind of a mission and product perspective, a business perspective, an \\r\\ninvestment time frame perspective. But I also understand the discomfort that some \\r\\nfolks have with something that I can't put exact numbers on a near-term time \\r\\nhorizon around.\\r\\nOperator: Your next question comes from the line of Doug Anmuth with JPMorgan.\\r\\nDoug Anmuth: One for Mark, one for Susan. Mark, you touched on it a bit, but can you just talk a \\r\\nlittle bit more about what you've learned from Threads early on, not just for that \\r\\nproduct specifically but also as you look to leverage the Family of Apps platform to \\r\\nlaunch additional products over time. And then, Susan, you lowered the CapEx \\r\\noutlook this year by $3 billion. Can you just provide more color on the delays in \\r\\nprojects and equipment in '23? And is there any way to frame how meaningful the \\r\\nCapEx increase could be in '24?\\r\\nMark Zuckerberg: Yes. On Threads, it's maybe too early to do this kind of analysis. I mean, I'm -- on the \\r\\none hand, we've tried a bunch of standalone experiences over time, and in general, \\r\\nwe haven't had a lot of success with building kind of standalone apps. The biggest \\r\\nexception to that, of course, is Messenger, but that started off as functionality inside \\r\\nFacebook and was spun out. So part of me wonders if this is just a kind of classic \\r\\nventure capital portfolio question, where you try a bunch of things and a bunch of \\r\\nthem don't work. And then every once in a while, one hits and is a much bigger \\r\\nsuccess. It could be that.\\n\", metadata={'page': 14, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}), -0.5471728672729328), (Document(page_content=\"4\\r\\nTo wrap up, I just want to say I'm really proud of our teams for everything we've accomplished so far \\r\\nthis year. It's been a tough year in a lot of ways, but it’s also been an impactful one. I'm quite optimistic \\r\\nabout the road ahead and grateful to you all for being on this journey with us. Now, I’m going to hand it \\r\\nover to Susan.\\r\\nSusan Li, CFO\\r\\nThanks Mark and good afternoon everyone.\\r\\nLet’s begin with our consolidated results. All comparisons are on a year-over-year basis unless otherwise \\r\\nnoted.\\r\\nQ2 total revenue was $32.0 billion, up 11% or 12% on a constant currency basis.\\r\\nQ2 total expenses were $22.6 billion, up 10% compared to last year.\\r\\nIn terms of the specific line items:\\r\\n• Cost of revenue increased 15%, driven primarily by infrastructure-related costs.\\r\\n• R&D increased 8%, driven mainly by headcount-related costs from our Reality Labs and Family \\r\\nof Apps segments as well as restructuring costs.\\r\\n• Marketing & Sales decreased 12%, due mostly to lower marketing spend and payroll-related \\r\\ncosts.\\r\\n• G&A increased 39%, due primarily to an increase in legal accruals which was partially offset by \\r\\nlower payroll-related costs.\\r\\nWe ended the second quarter with over 71,400 employees, down 7% from the first quarter. Our second \\r\\nquarter headcount still included roughly half of the approximately 10,000 employees impacted by the \\r\\n2023 layoffs. We expect that our third quarter headcount will no longer include the vast majority of \\r\\nimpacted employees.\\r\\nSecond quarter operating income was $9.4 billion, representing a 29% operating margin.\\r\\nOur tax rate for the quarter was 16%. This is lower than our previous full-year outlook as our higher \\r\\nshare price provided a higher tax deduction and lowered our taxes.\\r\\nNet income was $7.8 billion or $2.98 per share.\\r\\nCapital expenditures, including principal payments on finance leases, were $6.4 billion, driven by \\r\\ninvestments in data centers, servers and network infrastructure.\\r\\nFree cash flow was $11.0 billion, significantly benefitting from a deferral of income taxes that we expect \\r\\nwill be paid in the fourth quarter. We repurchased $793 million of our Class A common stock in the \\r\\nsecond quarter and ended the quarter with $53.4 billion in cash and marketable securities.\\r\\nMoving now to our segment results.\\r\\nI’ll begin with our Family of Apps segment.\\n\", metadata={'page': 3, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}), -0.5567319046077588), (Document(page_content=\"12\\r\\nOn the second question about the growth in Reality Labs operating losses in 2024, \\r\\nMark alluded to the fact that this is an ambitious long-term horizon multifaceted \\r\\nroadmap. There are lots of components to the Reality Labs portfolio across VR, AR, \\r\\nmetaverse social platforms, neural interfaces.\\r\\nAnd we really have a long-term time horizon for evaluating the return on our \\r\\ninvestments here. So in the near term, we're focused on growing adoption of the \\r\\nexisting products, and we're constantly learning more about demand and use cases \\r\\nthat inform our future plans. But a lot of the investment that's driving the growth \\r\\nhere is around conducting the fundamental R&D to solve hard technology problems \\r\\nthat are going to enable our vision here. A lot of it is around clearing technical \\r\\nhurdles that will make subsequent devices smaller, cost less, weigh less, et cetera. \\r\\nSo that's really on the Reality Labs side.\\r\\nAgain, as we said, we expect operating losses to increase meaningfully year-over\\ufffeyear in 2024. In 2024, specifically, I think that will be driven by a combination of \\r\\nboth headcount-related and operating costs. But again, our ambitions in Reality \\r\\nLabs haven't changed, and it continues to be a significant long-term opportunity for \\r\\nus.\\r\\nOperator: Your next question comes from the line of Mark Shmulik with Bernstein.\\r\\nMark Shmulik: Mark, things move quickly. Like a month ago, we weren't talking about Reels. A \\r\\nquarter ago, we weren't talking about Llama, and start of the year, we were barely \\r\\ntalking about AI. If you think back to kind of just how you were prioritizing your time \\r\\nat the start of the year to kind of where you are today and how that's kind of \\r\\nchanged, would love to just get some color on kind of the changing priorities and \\r\\nreally where you're spending your time.\\r\\nAnd then the second question, really impressive, obviously, acceleration of growth \\r\\nin kind of the core ad business. Would love to just get some color kind of beyond the \\r\\nvertical contribution. What are you seeing in terms of kind of adoption of \\r\\nAdvantage+ products really driving some of that improved growth, especially what \\r\\nwe've seen relative to the rest of the sector?\\r\\nMark Zuckerberg: Yes. I can start with the first and then Susan can take the second. I actually think our \\r\\npriorities have been pretty consistent for a few years now. I think the way that \\r\\npeople hear them might be different. But for example, last year, I think we were \\r\\ngetting quite a bit of critique for the volume of AI CapEx spending that we were \\r\\ndoing, and now obviously, people want to understand where that's going and where \\r\\nwe think that trajectory is going and want to make it as efficient as possible. But I \\r\\nthink at this point, it's more well understood that I think that was a good \\r\\ninvestment, and it's driving results in the near term and enabling us to build some of \\r\\nthe new experiences that I think we all think are pretty fundamental.\\r\\nI don't think Reels is new, although maybe you meant Threads when you said that.\\r\\nThreads, I would say it's not that -- Threads is like -- it's not a massive project. It was \\n\", metadata={'page': 11, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}), -0.5735935454549406), (Document(page_content='7\\r\\nsupporting these initiatives will come from prioritizing them against other areas of work and shifting \\r\\nresources. However, in some cases they will require incremental investment. This is particularly true in \\r\\nthe areas we see the most significant opportunity, which include AI and the metaverse.\\r\\nAs I mentioned last quarter, we also remain focused on modestly evolving our capital structure over \\r\\ntime. We were pleased to execute our second bond offering in May and expect a measured pace of \\r\\nfuture debt raises as we work toward improving our overall cost of capital while maintaining a positive \\r\\nor neutral net cash balance.\\r\\nIn addition, we continue to monitor the active regulatory landscape. With respect to EU-U.S. data \\r\\ntransfers, we saw a positive development with the European Commission’s adoption of a final adequacy \\r\\ndecision, which allows us to continue to provide our services in Europe. This is good news, though \\r\\nbroadly speaking, we continue to see increasing legal and regulatory headwinds in the EU and the US \\r\\nthat could significantly impact our business and our financial results.\\r\\nTurning now to the revenue outlook.\\r\\nWe expect third quarter 2023 total revenue to be in the range of $32-34.5 billion. Our guidance assumes \\r\\na foreign currency tailwind of approximately 3% to year-over-year total revenue growth in the third \\r\\nquarter, based on current exchange rates.\\r\\nTurning now to the expense outlook.\\r\\nWe anticipate that our full-year 2023 total expenses will be in the range of $88-91 billion, increased \\r\\nfrom our prior range of $86-90 billion due to legal-related expenses recorded in Q2. This outlook \\r\\nincludes approximately $4 billion of restructuring costs related to facilities consolidation charges and \\r\\nseverance and other personnel costs. We expect Reality Labs operating losses to increase year-over-year \\r\\nin 2023.\\r\\nWhile we are not providing a quantitative outlook beyond 2023 at this point, we expect a few factors to \\r\\nbe drivers of total expense growth in 2024 as we continue to invest in our most compelling \\r\\nopportunities, including AI and the metaverse.\\r\\n• First, we expect higher infrastructure-related costs next year. Given our increased capital \\r\\ninvestments in recent years, we expect depreciation expenses in 2024 to increase by a larger \\r\\namount than in 2023. We also expect to incur higher operating costs from running a larger \\r\\ninfrastructure footprint.\\r\\n• Second, we anticipate growth in payroll expenses as we evolve our workforce composition \\r\\ntoward higher-cost technical roles.\\r\\n• Finally, for Reality Labs, we expect operating losses to increase meaningfully year-over-year due \\r\\nto our ongoing product development efforts in AR/VR and our investments to further scale our \\r\\necosystem.\\r\\nTurning now to the capex outlook.\\r\\nWe expect capital expenditures to be in the range of $27-30 billion, lowered from our prior estimate of\\r\\n$30-33 billion. The reduced forecast is due to both cost savings, particularly on non-AI servers, as well as \\r\\nshifts in capex into 2024 from delays in projects and equipment deliveries rather than a reduction in \\r\\noverall investment plans.\\n', metadata={'page': 6, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}), -0.6116797233037645)]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for d in coll1.similarity_search_with_relevance_scores(\"who was sita\",k=99):\n",
    "    print(\n",
    "          d[0].metadata,d[1])\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3bf6130a-4a73-4466-93da-7b67ea4b6e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coll1.as_retriever??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0622c463-dc10-4b4d-926b-b6f1da1d745f",
   "metadata": {},
   "source": [
    "import chromadb\n",
    "\n",
    "client=chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e20fcb06-3ef9-4c31-923b-8538c5fdecef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coll1\n"
     ]
    }
   ],
   "source": [
    "for coll in client.list_collections():\n",
    "    print(coll.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89e12b0-0e62-4298-869a-cb86f52cb49f",
   "metadata": {},
   "source": [
    "client.get_collection(\"langchain\").name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180bb394-78a5-4f11-8f12-5b2a3fa8a982",
   "metadata": {},
   "source": [
    "## date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f35dc7-7add-4468-932e-4b74c2f73cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d44d1-b2b2-4267-90eb-64f0f3f66988",
   "metadata": {},
   "outputs": [],
   "source": [
    "now=datetime.now().isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1c2c71-dd4c-4a6f-a4fa-9ff9bf75a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae0fd06-3a25-4bd3-bcb5-b40c977b19d5",
   "metadata": {},
   "source": [
    "# QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4b3fb7e-cb97-4b83-b2de-29f59fd84224",
   "metadata": {},
   "outputs": [],
   "source": [
    "coll1.persist()\n",
    "del(coll1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d55eff9-9edb-427c-8d11-f0e12c9a8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "coll1 = Chroma(\"Coll1\",embeddings,persist_directory=\"db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1012a2b0-da61-44c7-b5fd-342930d07d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coll1.get()['ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0d29cc97-5453-4b81-91be-19ef6a333622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatLiteLLM\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "chain =load_qa_chain(ChatLiteLLM(model=\"together_ai/togethercomputer/llama-2-70b-chat\",temperature=0), chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "125025ea-4b10-499d-b96f-5a4cf48e82f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retr=coll1.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "705c4127-d5d9-4118-bb66-5cba3e7c0af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\litellm\\utils.py:4528\u001b[0m, in \u001b[0;36mexception_type\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[0;32m   4522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(\n\u001b[0;32m   4523\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTogetherAIException - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   4524\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   4525\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtogether_ai\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   4526\u001b[0m         response\u001b[38;5;241m=\u001b[39moriginal_exception\u001b[38;5;241m.\u001b[39mresponse\n\u001b[0;32m   4527\u001b[0m     )\n\u001b[1;32m-> 4528\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m408\u001b[39m:\n\u001b[0;32m   4529\u001b[0m         exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AttributeError' object has no attribute 'status_code'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m question\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the name of company?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m docs\u001b[38;5;241m=\u001b[39mretr\u001b[38;5;241m.\u001b[39mget_relevant_documents(question)\n\u001b[1;32m----> 4\u001b[0m chain\u001b[38;5;241m.\u001b[39mrun(input_documents\u001b[38;5;241m=\u001b[39mdocs, question\u001b[38;5;241m=\u001b[39mquestion)\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\langchain\\chains\\base.py:510\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    506\u001b[0m         _output_key\n\u001b[0;32m    507\u001b[0m     ]\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m--> 510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    511\u001b[0m         _output_key\n\u001b[0;32m    512\u001b[0m     ]\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    518\u001b[0m     )\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\langchain\\chains\\base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    311\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    312\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    314\u001b[0m )\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\langchain\\chains\\base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    297\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    298\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    299\u001b[0m     inputs,\n\u001b[0;32m    300\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    301\u001b[0m )\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 304\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    307\u001b[0m     )\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:122\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    121\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[1;32m--> 122\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[0;32m    123\u001b[0m     docs, callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_keys\n\u001b[0;32m    124\u001b[0m )\n\u001b[0;32m    125\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py:171\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[1;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs), {}\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\langchain\\chains\\llm.py:298\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    284\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \n\u001b[0;32m    286\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\langchain\\chains\\base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    311\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    312\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    314\u001b[0m )\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\langchain\\chains\\base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    297\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    298\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    299\u001b[0m     inputs,\n\u001b[0;32m    300\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    301\u001b[0m )\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 304\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    307\u001b[0m     )\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\langchain\\chains\\llm.py:108\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    105\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    106\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    107\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 108\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate([inputs], run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\langchain\\chains\\llm.py:120\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    118\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    121\u001b[0m         prompts,\n\u001b[0;32m    122\u001b[0m         stop,\n\u001b[0;32m    123\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs,\n\u001b[0;32m    125\u001b[0m     )\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    127\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    128\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    129\u001b[0m     )\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\langchain\\chat_models\\base.py:459\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    453\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    457\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    458\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\langchain\\chat_models\\base.py:349\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    348\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    350\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    351\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    353\u001b[0m ]\n\u001b[0;32m    354\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\langchain\\chat_models\\base.py:339\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 339\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    340\u001b[0m                 m,\n\u001b[0;32m    341\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    342\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    343\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    344\u001b[0m             )\n\u001b[0;32m    345\u001b[0m         )\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\langchain\\chat_models\\base.py:492\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    489\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    490\u001b[0m     )\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[1;32m--> 492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    493\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    494\u001b[0m     )\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\langchain\\chat_models\\litellm.py:306\u001b[0m, in \u001b[0;36mChatLiteLLM._generate\u001b[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    305\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m--> 306\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_with_retry(\n\u001b[0;32m    307\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    308\u001b[0m )\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\langchain\\chat_models\\litellm.py:239\u001b[0m, in \u001b[0;36mChatLiteLLM.completion_with_retry\u001b[1;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcompletion(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _completion_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\langchain\\chat_models\\litellm.py:237\u001b[0m, in \u001b[0;36mChatLiteLLM.completion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcompletion(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\litellm\\utils.py:1482\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1478\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1479\u001b[0m             liteDebuggerClient \u001b[38;5;129;01mand\u001b[39;00m liteDebuggerClient\u001b[38;5;241m.\u001b[39mdashboard_url \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1480\u001b[0m         ):  \u001b[38;5;66;03m# make it easy to get to the debugger logs if you've initialized it\u001b[39;00m\n\u001b[0;32m   1481\u001b[0m             e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Check the log in your dashboard - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mliteDebuggerClient\u001b[38;5;241m.\u001b[39mdashboard_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1482\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\litellm\\utils.py:1411\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1409\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m cached_result\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[1;32m-> 1411\u001b[0m result \u001b[38;5;241m=\u001b[39m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1412\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1414\u001b[0m     \u001b[38;5;66;03m# TODO: Add to cache for streaming\u001b[39;00m\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\litellm\\main.py:1425\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, deployment_id, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   1422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m   1423\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[1;32m-> 1425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[0;32m   1426\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel, custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider, original_exception\u001b[38;5;241m=\u001b[39me, completion_kwargs\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1427\u001b[0m         )\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\litellm\\utils.py:4765\u001b[0m, in \u001b[0;36mexception_type\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[0;32m   4763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   4764\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m original_exception\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\litellm\\main.py:1047\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, deployment_id, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m api_base \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1036\u001b[0m     api_base\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mapi_base\n\u001b[0;32m   1038\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m get_secret(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOGETHERAI_API_BASE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.together.xyz/inference\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1040\u001b[0m )\n\u001b[0;32m   1042\u001b[0m custom_prompt_dict \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1043\u001b[0m     custom_prompt_dict\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mcustom_prompt_dict\n\u001b[0;32m   1045\u001b[0m )\n\u001b[1;32m-> 1047\u001b[0m model_response \u001b[38;5;241m=\u001b[39m together_ai\u001b[38;5;241m.\u001b[39mcompletion(\n\u001b[0;32m   1048\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1049\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m   1050\u001b[0m     api_base\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[0;32m   1051\u001b[0m     model_response\u001b[38;5;241m=\u001b[39mmodel_response,\n\u001b[0;32m   1052\u001b[0m     print_verbose\u001b[38;5;241m=\u001b[39mprint_verbose,\n\u001b[0;32m   1053\u001b[0m     optional_params\u001b[38;5;241m=\u001b[39moptional_params,\n\u001b[0;32m   1054\u001b[0m     litellm_params\u001b[38;5;241m=\u001b[39mlitellm_params,\n\u001b[0;32m   1055\u001b[0m     logger_fn\u001b[38;5;241m=\u001b[39mlogger_fn,\n\u001b[0;32m   1056\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   1057\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mtogether_ai_key,\n\u001b[0;32m   1058\u001b[0m     logging_obj\u001b[38;5;241m=\u001b[39mlogging,\n\u001b[0;32m   1059\u001b[0m     custom_prompt_dict\u001b[38;5;241m=\u001b[39mcustom_prompt_dict\n\u001b[0;32m   1060\u001b[0m )\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m optional_params \u001b[38;5;129;01mand\u001b[39;00m optional_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;66;03m# don't try to access stream object,\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m     response \u001b[38;5;241m=\u001b[39m CustomStreamWrapper(\n\u001b[0;32m   1064\u001b[0m         model_response, model, custom_llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtogether_ai\u001b[39m\u001b[38;5;124m\"\u001b[39m, logging_obj\u001b[38;5;241m=\u001b[39mlogging\n\u001b[0;32m   1065\u001b[0m     )\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\litellm\\llms\\together_ai.py:118\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, api_base, model_response, print_verbose, encoding, api_key, logging_obj, custom_prompt_dict, optional_params, litellm_params, logger_fn)\u001b[0m\n\u001b[0;32m    109\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m custom_prompt(\n\u001b[0;32m    110\u001b[0m             role_dict\u001b[38;5;241m=\u001b[39mmodel_prompt_details\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroles\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}), \n\u001b[0;32m    111\u001b[0m             initial_prompt_value\u001b[38;5;241m=\u001b[39mmodel_prompt_details\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_prompt_value\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m             messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m    116\u001b[0m         )\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m prompt_factory(model\u001b[38;5;241m=\u001b[39mmodel, messages\u001b[38;5;241m=\u001b[39mmessages, api_key\u001b[38;5;241m=\u001b[39mapi_key, custom_llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtogether_ai\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# api key required to query together ai model list\u001b[39;00m\n\u001b[0;32m    120\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage-model-inference\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptional_params,\n\u001b[0;32m    125\u001b[0m }\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\litellm\\llms\\prompt_templates\\factory.py:373\u001b[0m, in \u001b[0;36mprompt_factory\u001b[1;34m(model, messages, custom_llm_provider, api_key)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtogether_ai\u001b[39m\u001b[38;5;124m\"\u001b[39m: \n\u001b[0;32m    372\u001b[0m     prompt_format, chat_template \u001b[38;5;241m=\u001b[39m get_model_info(token\u001b[38;5;241m=\u001b[39mapi_key, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m--> 373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m format_prompt_togetherai(messages\u001b[38;5;241m=\u001b[39mmessages, prompt_format\u001b[38;5;241m=\u001b[39mprompt_format, chat_template\u001b[38;5;241m=\u001b[39mchat_template)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/llama-2\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model:\n",
      "File \u001b[1;32mC:\\sw\\py311nb_env\\Lib\\site-packages\\litellm\\llms\\prompt_templates\\factory.py:283\u001b[0m, in \u001b[0;36mformat_prompt_togetherai\u001b[1;34m(messages, prompt_format, chat_template)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_prompt_togetherai\u001b[39m(messages, prompt_format, chat_template):\n\u001b[1;32m--> 283\u001b[0m     human_prompt, assistant_prompt \u001b[38;5;241m=\u001b[39m prompt_format\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{prompt}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chat_template \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    286\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m hf_chat_template(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, messages\u001b[38;5;241m=\u001b[39mmessages, chat_template\u001b[38;5;241m=\u001b[39mchat_template)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "question=\"What is the name of company?\"\n",
    "docs=retr.get_relevant_documents(question)\n",
    "\n",
    "chain.run(input_documents=docs, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9e7d9bc9-2cc1-4d83-917c-c79d726e1e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page': 3, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'},\n",
       " {'page': 18, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'},\n",
       " {'page': 4, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'},\n",
       " {'page': 7, 'source': 'c:/temp/META-Q2-2023-Earnings-Call-Transcript.pdf'}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d.metadata for d in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa81661-c390-44fa-8236-3afb58c939f0",
   "metadata": {},
   "source": [
    "## BARD snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb65f60-29d5-429d-8a7d-b6a6a17922aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Chroma\n",
    "from langchain.documents import Document\n",
    "from util_misc\n",
    "\n",
    "def load_document_in_chunks(document_path, collection_name=\"default_collection\"):\n",
    "    \"\"\"\n",
    "    Loads a document into Chroma DB in chunks of 10%.\n",
    "\n",
    "    Args:\n",
    "        document_path: The path to the document file.\n",
    "        collection_name: The name of the collection to store the document in.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(document_path, 'r') as f:\n",
    "        document_text = f.read()\n",
    "\n",
    "    # Calculate chunk size based on 10% of the document text\n",
    "    chunk_size = int(len(document_text) * 0.1)\n",
    "\n",
    "    # Initialize Chroma client\n",
    "    client = Chroma()\n",
    "\n",
    "    # Create the collection if it doesn't exist\n",
    "    client.create_collection(collection_name)\n",
    "\n",
    "    # Load document in chunks\n",
    "    for i in range(0, len(document_text), chunk_size):\n",
    "        chunk = document_text[i:i + chunk_size]\n",
    "        document = Document(content=chunk)\n",
    "        client.insert_documents([document], collection_name)\n",
    "\n",
    "    print(f\"Document loaded in Chroma DB collection: {collection_name}\")\n",
    "\n",
    "# Example usage\n",
    "load_document_in_chunks(\"path/to/your/document.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
